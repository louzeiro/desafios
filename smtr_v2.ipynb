{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smtr_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3wM6yPt7UoXofRIsYykl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louzeiro/desafios/blob/main/smtr_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prefect\n",
        "\n",
        "Tutorial seguido:<br>\n",
        "https://towardsdatascience.com/prefect-how-to-write-and-schedule-your-first-etl-pipeline-with-python-54005a34f10b"
      ],
      "metadata": {
        "id": "Y6Wuc6fzzqBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system('pip3 install prefect==1.2.4')"
      ],
      "metadata": {
        "id": "tSGVLqlmsgd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import prefect\n",
        "from prefect import task, Flow, Parameter\n",
        "from prefect.schedules import IntervalSchedule\n",
        "\n",
        "\n",
        "\n",
        "tz = pytz.timezone('America/Sao_Paulo')\n",
        "\n",
        "scheduler = IntervalSchedule(\n",
        "    interval=timedelta(seconds=10)\n",
        ")\n",
        "var_downtime=60\n"
      ],
      "metadata": {
        "id": "yj3EkPkqu1jb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task(max_retries=10, retry_delay=timedelta(seconds=10)) # realizar tentativas em 10 segundos\n",
        "def extract(url: str) -> dict:\n",
        "    res = requests.get(url)\n",
        "    if not res:\n",
        "        raise Exception('Sem dados!')\n",
        "    return json.loads(res.content)"
      ],
      "metadata": {
        "id": "7R-cpCpuzlYM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def transform(data: dict) -> pd.DataFrame:\n",
        "    instante = datetime.now(tz=tz)\n",
        "    transformed = []\n",
        "    for veiculo in data:\n",
        "        transformed.append({\n",
        "            'longitude': veiculo['longitude'],\n",
        "            'latitude': veiculo['latitude'],\n",
        "            'vei_nro_gestor': veiculo['vei_nro_gestor'],\n",
        "            'direcao': veiculo['direcao'],\n",
        "            'velocidade': veiculo['velocidade'],\n",
        "            'inicio_viagem': veiculo['inicio_viagem'],\n",
        "            'linha': veiculo['linha'],\n",
        "            'nomeLinha': veiculo['nomeLinha'],\n",
        "            'nomeItinerario': veiculo['nomeItinerario'],\n",
        "            'comunicacao': veiculo['comunicacao'],\n",
        "             'info_coleta': instante\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(transformed)"
      ],
      "metadata": {
        "id": "VNGlV7rizsJS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def clear_data(data_aux: pd.DataFrame,\n",
        "               data_updated: pd.DataFrame,\n",
        "               var_downtime: int) -> pd.DataFrame:\n",
        "  \n",
        "  if (data_updated.empty): # verificando se o dataframe foi alimentado\n",
        "    return data_aux        # retorna o dataframe auxiliar\n",
        "  \n",
        "  else:\n",
        "    #var_downtime = 60\n",
        "    for idx in data_aux.index:\n",
        "      # verificando se o veículo está no dataframe antigo\n",
        "      if not data_aux.vei_nro_gestor[idx] in data_updated.vei_nro_gestor.values:\n",
        "        new_vehicle = data_aux.loc[data_aux.vei_nro_gestor==data_aux.vei_nro_gestor[idx]]\n",
        "        data_updated = data_updated.append(new_vehicle, ignore_index=True)\n",
        "        #print(f'O veículo {str(new_vehicle.vei_nro_gestor.values)} foi adicionado ao dataframe')\n",
        "      \n",
        "      # O veiculo já está no dataframe atualizado\n",
        "      else:\n",
        "        # salvando a sua posição no dataframe anterior\n",
        "        idx_vehicle = data_updated[data_updated['vei_nro_gestor']==data_aux['vei_nro_gestor'][idx]].index.values\n",
        "\n",
        "        # verificando se houve atualização nas coordenadas do veículo\n",
        "        if(data_updated.loc[idx_vehicle].longitude.values !=data_aux.loc[idx].longitude or\n",
        "           data_updated.loc[idx_vehicle].latitude.values !=data_aux.loc[idx].latitude):\n",
        "          #atualizando no dataframe as infos do veiculo\n",
        "          data_updated.loc[idx_vehicle] = data_aux.loc[idx].values\n",
        "          #print(f'O veículo {str(data_updated.loc[idx_vehicle].vei_nro_gestor.values)} está em circulação e suas informações foram atualizadas')\n",
        "\n",
        "        # não houve atualização nas coordenadas, verificar o tempo parado           \n",
        "        else:\n",
        "          downtime = data_aux.loc[idx].info_coleta - data_updated.loc[idx_vehicle].info_coleta\n",
        "          if (float(downtime.dt.total_seconds()) > var_downtime):\n",
        "            print(f'O veículo {str(data_updated.loc[idx_vehicle].vei_nro_gestor.values)} está parado a {float(downtime.dt.total_seconds())} segundos, então foi removido do dataframe')\n",
        "            data_updated.drop(idx_vehicle, axis=0, inplace=True)\n",
        "    \n",
        "    return data_updated\n"
      ],
      "metadata": {
        "id": "ZTjs6V0gV4qo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def load(data: pd.DataFrame, path: str) -> None:\n",
        "    data.to_csv(path_or_buf=path, index=False)"
      ],
      "metadata": {
        "id": "92qqA7ha2qqH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prefect_flow(data_updated: pd.DataFrame, var_downtime: int):\n",
        "    with Flow(name='smtr_veiculos_etl', schedule=scheduler) as flow:\n",
        "\n",
        "        veiculos = extract(url='http://citgisbrj.tacom.srv.br:9977/gtfs-realtime-exporter/findAll/json')\n",
        "        data_aux = transform(veiculos)\n",
        "\n",
        "        data_updated = clear_data(data_aux=data_aux,\n",
        "                                  data_updated = data_updated,\n",
        "                                  var_downtime = var_downtime)\n",
        "        \n",
        "        load(data=data_updated, path=f'veiculos_{datetime.now(tz=tz).strftime(\"%Y-%m-%d_%H:%M:%S.%f\")[:-3]}.csv')\n",
        "    \n",
        "    return flow"
      ],
      "metadata": {
        "id": "aHRlsSzd1H6r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  data_updated = pd.DataFrame()  # iniciando um df vazio\n",
        "  flow = prefect_flow(data_updated=data_updated, var_downtime=var_downtime)\n",
        "  flow.run()"
      ],
      "metadata": {
        "id": "46G3xeiK52nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flow.visualize()"
      ],
      "metadata": {
        "id": "qHobUDeRVDJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%rm *.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu7IJbsRJTVS",
        "outputId": "2c946c4c-4aee-4eb7-8736-b6e5dfd60a4a"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# aprendendo"
      ],
      "metadata": {
        "id": "bcxW4DtGzXTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "veiculos = extract(url='http://citgisbrj.tacom.srv.br:9977/gtfs-realtime-exporter/findAll/json')\n",
        "data_updated = transform(veiculos)\n",
        "time.sleep(65)\n",
        "veiculos = extract(url='http://citgisbrj.tacom.srv.br:9977/gtfs-realtime-exporter/findAll/json')\n",
        "data_aux = transform(veiculos)\n",
        "\n",
        "var_downtime = 60\n",
        "df = clear_data(data_updated = data_updated, \n",
        "                data_aux=data_aux, \n",
        "                var_downtime=var_downtime)\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "6238qHKZzWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "veiculos = extract(url='http://citgisbrj.tacom.srv.br:9977/gtfs-realtime-exporter/findAll/json')\n",
        "data_updated = transform(veiculos)\n",
        "time.sleep(65)\n",
        "veiculos = extract(url='http://citgisbrj.tacom.srv.br:9977/gtfs-realtime-exporter/findAll/json')\n",
        "data_aux = transform(veiculos)\n",
        "\n",
        "var_downtime = 60\n",
        "for idx in data_aux.index:\n",
        "  # verificando se o veículo está no dataframe antigo\n",
        "  if not data_aux.vei_nro_gestor[idx] in data_updated.vei_nro_gestor.values:\n",
        "    new_vehicle = data_aux.loc[data_aux.vei_nro_gestor==data_aux.vei_nro_gestor[idx]]\n",
        "    data_updated = data_updated.append(new_vehicle, ignore_index=True)\n",
        "    print(f'O veículo {str(new_vehicle.vei_nro_gestor.values)} foi adicionado ao dataframe')\n",
        "  \n",
        "  # O veiculo já está no dataframe atualizado\n",
        "  else:\n",
        "    idx_vehicle = data_updated[data_updated['vei_nro_gestor']==data_aux['vei_nro_gestor'][idx]].index.values\n",
        "\n",
        "    # verificando se houve atualização nas coordenadas do veículo\n",
        "    if(data_updated.loc[idx_vehicle].longitude.values !=data_aux.loc[idx].longitude or\n",
        "           data_updated.loc[idx_vehicle].latitude.values !=data_aux.loc[idx].latitude):\n",
        "        \n",
        "      #atualizando no dataframe as infos do veiculo\n",
        "      data_updated.loc[idx_vehicle] = data_aux.loc[idx].values\n",
        "      print(f'O veículo {str(data_updated.loc[idx_vehicle].vei_nro_gestor.values)} está em circulação e suas informações foram atualizadas')\n",
        "\n",
        "    # não houve atualização nas coordenadas, verificar o tempo parado           \n",
        "    else:\n",
        "      downtime = data_aux.loc[idx].info_coleta - data_updated.loc[idx_vehicle].info_coleta\n",
        "      if (float(downtime.dt.total_seconds()) > var_downtime):\n",
        "        print(f'O veículo {str(data_updated.loc[idx_vehicle].vei_nro_gestor.values)} está parado a {float(downtime.dt.total_seconds())} segundos, então foi removido do dataframe')\n",
        "        data_updated.drop(idx_vehicle, axis=0, inplace=True)\n",
        "        "
      ],
      "metadata": {
        "id": "NFh2B6cXdcjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account"
      ],
      "metadata": {
        "id": "GmblU-aeDh3g"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fazer upload da chave e renomear\n",
        "credentials = service_account.Credentials.from_service_account_file(\"smtr-gbq.json\", \n",
        "                                                                    scopes=[\"https://www.googleapis.com/auth/cloud-plataform\"])\n",
        "credentials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hml0d1_Fgbm",
        "outputId": "d114bc9e-1111-430d-f02e-c48dd0965a8b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.oauth2.service_account.Credentials at 0x7f21192c3bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_updated.to_gbq(credentials=credentials,\n",
        "                    destination_table='smtr.prova',\n",
        "                    if_exists='append') #substitue caso já exista a tabela, não é o meu caso "
      ],
      "metadata": {
        "id": "tEYhDUGIA8mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "2nXeKStyNKKF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'smtr-359700'"
      ],
      "metadata": {
        "id": "GABEKCLSNZcS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery"
      ],
      "metadata": {
        "id": "5oKn1ppNNnwB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "xTcVx4zVNsS-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ref = "
      ],
      "metadata": {
        "id": "hI7nUM5hN7zh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}